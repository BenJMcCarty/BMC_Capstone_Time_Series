{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cancel Culture - Classification Modeling Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Post-Cleaning Modeling Notebook**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- > ðŸ›‘ **FIX**: Add cmts re: post-cleaning, modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The Basics**\n",
    "\n",
    ">I will import the usual packages: Pandas, Numpy, Matplotlib, and Seaborn. Additionally, I have several personal functions that I use during the modeling process.\n",
    "\n",
    "**More Models**\n",
    "\n",
    "> When I begin the modeling process, I will import \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:24.514558Z",
     "start_time": "2021-09-08T21:06:24.448559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Jupyter Notebook setting to reload functions when called\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:29.881557Z",
     "start_time": "2021-09-08T21:06:24.517560Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Visualizing model results\n",
    "import shap\n",
    "\n",
    "## Personal functions\n",
    "from bmc_functions import classification as clf\n",
    "\n",
    "## SKLearn and Modeling Tools\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,\\\n",
    "                                    RepeatedStratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:30.137558Z",
     "start_time": "2021-09-08T21:06:29.884558Z"
    }
   },
   "outputs": [],
   "source": [
    "## Settings\n",
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.25)\n",
    "\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 50)\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Speeding-Up Scikit-Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Due to the size of my dataset, the modeling process took a fair amount of time, especially when testing different model types. To improve my models' runtime, I use a package called \"**Intel(R) Extension for Scikit-learn*.**\"\n",
    "\n",
    "This package operates in the background to increase the computational efficiency of certain Scikit-Learn models, including Logistic Regression and Random Forest Classifier models. The package does not affect the model results, though.\n",
    "\n",
    "This package requires the models to be imported after the package itself in order to perform the patching that results in better run-times.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:30.457557Z",
     "start_time": "2021-09-08T21:06:30.139559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Speeding up SKLearn via Intel(R) Extension for Scikit-learn*\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:30.745557Z",
     "start_time": "2021-09-08T21:06:30.459560Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inmporting models post-sklearn-intelex\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading the DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In my prior EDA notebook, I reviewed, cleaned, and performed some pre-processing steps to prepare my data separately before modeling. I saved the data as a .pickle file to preserve the datatypes; now I will re-read the data for modeling purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:31.313559Z",
     "start_time": "2021-09-08T21:06:30.747559Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/data_prepped.pickle',\n",
    "                           compression = 'gzip')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:31.581558Z",
     "start_time": "2021-09-08T21:06:31.315560Z"
    }
   },
   "outputs": [],
   "source": [
    "## Identifying target\n",
    "target= 'is_canceled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:31.881558Z",
     "start_time": "2021-09-08T21:06:31.584560Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping target and \"reservation_status\" (nearly identical indicator)\n",
    "\n",
    "X = data.drop(columns = [target, 'reservation_status']).copy()\n",
    "y = data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:32.320558Z",
     "start_time": "2021-09-08T21:06:31.884559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking for missing values\n",
    "print(f'Missing values for X:\\n {X.isna().sum()[X.isna().sum() >0]}\\n')\n",
    "print(f'Missing values for y: {y.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:32.713560Z",
     "start_time": "2021-09-08T21:06:32.322559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting - stratify to maintain class balance b/t X_train/_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:32.985557Z",
     "start_time": "2021-09-08T21:06:32.715559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving memory by deleting unused X, y\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:33.237558Z",
     "start_time": "2021-09-08T21:06:32.987559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "num_cols = X_train.select_dtypes('number').columns.to_list()\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:33.537559Z",
     "start_time": "2021-09-08T21:06:33.239559Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.to_list()\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepping the Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Pipeline to streamline modeling steps:\n",
    "* Preprocessing: OHE, scaling, outliers via Æ’-XF?\n",
    "* Modeling: RFC, BRFC\n",
    "* GSCV: include as part of pipeline\n",
    "* Get results:\n",
    "    * Feature importances - **SHAP**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:33.833560Z",
     "start_time": "2021-09-08T21:06:33.539560Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "### --- Creating column pipelines --- ###\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown='ignore',\n",
    "                                                 sparse=False))])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "## Instantiating the ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipe, num_cols),\n",
    "                  ('cat', cat_pipe, cat_cols)\n",
    "                  ])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:35.630557Z",
     "start_time": "2021-09-08T21:06:33.836560Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cat'].named_steps['ohe'].get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]\n",
    "\n",
    "final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:46.395559Z",
     "start_time": "2021-09-08T21:06:44.968562Z"
    }
   },
   "outputs": [],
   "source": [
    "## Transform the data via the ColumnTransformer preprocessor\n",
    "\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting Cross-Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To confirm my models' performance, I will use the `RepeatedStratifiedKFold` cross-validation technique with Scikit-Learn's  `cross_val_score()` function to validate each fitted model's performance.\n",
    ">\n",
    "> I set the `cv` variable prior to modeling for the convenience during repeated cross-validations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:06:50.366560Z",
     "start_time": "2021-09-08T21:06:50.118560Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Due to class imbalance, will attempt to use \"class_weight = balanced\" to correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.5\n",
    "> \n",
    "> Testing balanced accuracy score: 0.51\n",
    "> \n",
    "> * *The training score is smaller by 0.01 points.*\n",
    ">\n",
    "> Training data log loss: 16.06\n",
    ">\n",
    "> Testing data log loss: 15.89\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:07:09.552561Z",
     "start_time": "2021-09-08T21:07:07.864565Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "base.fit(X_train_tf_df, y_train)\n",
    "\n",
    "clf.evaluate_classification(base,X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test, \n",
    "                           metric = 'balanced accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.82\n",
    "> \n",
    "> Testing balanced accuracy score: 0.82\n",
    "> \n",
    "> * *The scores are the same size.*\n",
    ">\n",
    "> Training data log loss: 0.37\n",
    ">\n",
    "> Testing data log loss: 0.37\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:07:32.315564Z",
     "start_time": "2021-09-08T21:07:16.472566Z"
    }
   },
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "logreg = LogisticRegression(max_iter = 600,C = .1,solver = 'lbfgs',\n",
    "                            random_state = 42)\n",
    "\n",
    "logreg.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:07:34.416566Z",
     "start_time": "2021-09-08T21:07:32.318567Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(logreg, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:29.026570Z",
     "start_time": "2021-09-08T21:07:34.419566Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing cross-validation to confirm results\n",
    "\n",
    "accuracy = cross_val_score(logreg, X_train_tf_df, y_train,\n",
    "                           scoring='balanced_accuracy', cv = cv)\n",
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collecting Coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T14:26:58.848264Z",
     "start_time": "2021-09-08T14:26:58.725265Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "> Based on the consistent cross-validation scores, I feel confident in my model's balanced accuracy score. Now I will collect the results for my features and generate a visualization of the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:29.324571Z",
     "start_time": "2021-09-08T21:08:29.028571Z"
    }
   },
   "outputs": [],
   "source": [
    "## Collecting coefficients for each feature as a Series\n",
    "lr_coefs = pd.Series(logreg.coef_.flatten(), index=X_train_tf_df.columns)\n",
    "lr_coefs.sort_values(ascending=False, inplace=True)\n",
    "lr_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:29.628569Z",
     "start_time": "2021-09-08T21:08:29.327572Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting top/bottom 5 values into a Series\n",
    "log_odds = pd.concat([lr_coefs.head(5), lr_coefs.tail(5)])\n",
    "log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:29.896569Z",
     "start_time": "2021-09-08T21:08:29.630571Z"
    }
   },
   "outputs": [],
   "source": [
    "## Formatting index labels to become visualization labels\n",
    "new_labels_list = [i.replace('_', ' ').title() for i in list(log_odds.index)]\n",
    "new_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:30.164569Z",
     "start_time": "2021-09-08T21:08:29.898570Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating a dictionary to replace the old lables with the new ones\n",
    "new_labels_dict = { k:v for (k,v) in zip(log_odds.index, new_labels_list)}\n",
    "new_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:30.448571Z",
     "start_time": "2021-09-08T21:08:30.166571Z"
    }
   },
   "outputs": [],
   "source": [
    "## Renaming Series index\n",
    "log_odds = log_odds.rename(new_labels_dict)\n",
    "log_odds.sort_values(inplace=True)\n",
    "\n",
    "log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:31.032571Z",
     "start_time": "2021-09-08T21:08:30.450572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Visualizing log-odds\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "ax = log_odds.plot(kind='barh', ax=ax)\n",
    "ax.axvline(linestyle = '-', c='k')\n",
    "ax.set_xlabel('Log-Odds')\n",
    "ax.set_ylabel('Feature Name')\n",
    "fig.suptitle('Top and Bottom Five Features')\n",
    "ax.set_facecolor('0.9')\n",
    "fig.set_facecolor('0.975')\n",
    "# plt.savefig('./img/log_odds.png',transparent=False, bbox_inches='tight',\n",
    "#            dpi=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***May the (Log-)Odds be Ever in Your Favor***\n",
    "\n",
    "> Based on the logistic regression model coefficients, I see that reservations are **most likely to cancel** if they:\n",
    "* Require non-refundable deposits (possibly 3rd-party booking sites like )\n",
    "* Are booked by agents 17 or 240\n",
    "* Have previous cancellations\n",
    "* Guests are from Portugal ([PRT is the three letter ISO 3166-1 code for Portugal](https://en.wikipedia.org/wiki/Portugal#:~:text=ISO%203166%20code,PT))\n",
    ">\n",
    "> Alternatively, reservations are **least likely to cancel** if they:\n",
    "* Do NOT require a deposit\n",
    "* Require parking spaces\n",
    "* Reserve room type \"A\"\n",
    "* Are assigned to room type \"I\"\n",
    "* Are booked by agent 152\n",
    "\n",
    "***Oddities in the Results***\n",
    "\n",
    "> **Non-Refundable Deposit Requirement**\n",
    "* May be associated with 3rd party travel groups like Priceline/Expedia/etc.\n",
    "    * Often require pre-payment to the booking group/agent to confirm booking\n",
    ">\n",
    "> **Country of Origin: Portugal**\n",
    "* Could these hotels be located in Portugal and have a larger percentage of domestic travelers?\n",
    ">\n",
    "> **Room Assignment, Required Car Parking Spaces**\n",
    "* These features may be generated post-stay and may not be available prior to arrival\n",
    "* Parking spot requirements may be specified prior to arrival, but not as likely in my personal experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 0.99\n",
    "> \n",
    "> Testing balanced recall score: 0.88\n",
    ">\n",
    "> * *The training score is larger by 0.11 points.*\n",
    ">\n",
    "> Training data log loss: 0.08\n",
    ">\n",
    "> Testing data log loss: 0.27\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:08:55.818125Z",
     "start_time": "2021-09-08T21:08:31.035571Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight = 'balanced',random_state=42)\n",
    "\n",
    "rfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:09:08.764127Z",
     "start_time": "2021-09-08T21:08:55.821128Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(rfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:09:09.265127Z",
     "start_time": "2021-09-08T21:09:08.766128Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(rfc, X_train_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:10:28.955135Z",
     "start_time": "2021-09-08T21:09:09.267128Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing cross-validation to confirm results\n",
    "\n",
    "accuracy = cross_val_score(rfc, X_train_tf_df, y_train,\n",
    "                           scoring='balanced_accuracy', cv = cv, n_jobs = -1)\n",
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ExtraTreesClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 1.0\n",
    "> \n",
    "> Testing balanced recall score: 0.87\n",
    "> \n",
    "> * \n",
    ">\n",
    "> Training data log loss: 0.01\n",
    ">\n",
    "> Testing data log loss: 0.33\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:12:21.572146Z",
     "start_time": "2021-09-08T21:10:28.957137Z"
    }
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(class_weight ='balanced',random_state=42)\n",
    "\n",
    "etc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:12:45.195151Z",
     "start_time": "2021-09-08T21:12:21.574148Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(etc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Balanced Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**MODEL: BalancedRandomForestClassifier**\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 0.97\n",
    "> \n",
    "> Testing balanced accuracy score: 0.89\n",
    "> \n",
    "> * *The training score is larger by 0.8 points.*\n",
    ">\n",
    "> Training data log loss: 0.17\n",
    ">\n",
    "> Testing data log loss: 0.30\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> \n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:13:35.891154Z",
     "start_time": "2021-09-08T21:12:45.197149Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brfc = BalancedRandomForestClassifier(random_state=42)\n",
    "\n",
    "brfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:13:56.951155Z",
     "start_time": "2021-09-08T21:13:35.893155Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(brfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:30.664165Z",
     "start_time": "2021-09-08T21:13:56.953158Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing cross-validation to confirm results\n",
    "accuracy = cross_val_score(brfc, X_train_tf_df, y_train,\n",
    "                           scoring='balanced_accuracy', cv = cv, n_jobs = -2)\n",
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interpreting Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›‘ FIX: adjust for context of only B/RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Odd Features**\n",
    "\n",
    "> Now that I completed my modeling steps, I will review the results of each model and determine my final recommendations.\n",
    ">\n",
    "> My main models are a standard logistic regression and a Balanced Random Forest classifier (\"BRFC\"). **Each model provides a different way of identifying which features are most impactful: logistic regressions provide \"log-odds\" and a Balanced Random Forest Classifier produces \"feature importances.\"** Both will require some processing for easy interpretation.\n",
    "\n",
    "**Feature Importances and SHAP**\n",
    "\n",
    "> As I mention above, tree-based models, including my BRFC-model, return \"feature importances\" instead of the coefficients associated with linear/logistic regressions. These values are useful to show the impact of a given feature on the decision-making steps of the tree model. \n",
    ">\n",
    ">However, these feature importances suffer from one key weakness: *they do not indicate if a feature increases or decreases the likelihood of a reservation canceling (my target feature).*\n",
    ">\n",
    "> I will utilize a visualization package called **SHAP** to produce \"Shapely values\" for each feature. These values indicate each feature's marginal contribution to the model - answering the question, \"*How well does the model perform with this feature than without?*\" \n",
    "\n",
    "**Seeing is Believing**\n",
    "\n",
    ">Using tools within the package, I will focus on two visualizations:\n",
    "> * The `summary_plot`: visualizing each feature's Shapely value and the feature's values from low-high (relative to each feature).\n",
    ">\n",
    ">\n",
    "> * The `force_plot`: an in-depth look at the forces impacting any given reservation record.\n",
    ">\n",
    "> More information about SHAP:\n",
    "* [SHAP Documentation](https://shap.readthedocs.io/en/latest/?badge=latest)\n",
    "* [SHAP Repository](https://github.com/slundberg/shap)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "> \n",
    "\n",
    "**Process**\n",
    "\n",
    "> \n",
    "\n",
    "**Performance**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›‘ Fix: Annotate Code and Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.567167Z",
     "start_time": "2021-09-08T21:15:30.666167Z"
    }
   },
   "outputs": [],
   "source": [
    "raise Exception('Hold for Testing SHAP Visualizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:43.816167Z",
     "start_time": "2021-09-08T21:15:43.544169Z"
    }
   },
   "outputs": [],
   "source": [
    " ## Initializing Javascript for SHAP models\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:48.201166Z",
     "start_time": "2021-09-08T21:15:47.942170Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generating a sample of the overall data for review:\n",
    "\n",
    "X_shap = shap.sample(X_test_tf_df, nsamples=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting TreeExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:17:39.134180Z",
     "start_time": "2021-09-08T21:17:38.695179Z"
    }
   },
   "outputs": [],
   "source": [
    "## Initializing an explainer with the RandomForestClassifier model\n",
    "t_explainer = shap.TreeExplainer(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T21:18:06.949Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculating SHAP values for test data\n",
    "shap_values = t_explainer.shap_values(X_shap,y_test)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting KernelExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.574166Z",
     "start_time": "2021-09-08T21:08:25.153Z"
    }
   },
   "outputs": [],
   "source": [
    "k_explainer = shap.KernelExplainer(rfc.predict, X_shap)\n",
    "k_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.575169Z",
     "start_time": "2021-09-08T21:08:25.157Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate shap values for test data\n",
    "shap_values = k_explainer.shap_values(X_shap, nsamples=100)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.576166Z",
     "start_time": "2021-09-08T21:08:25.162Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting sizes of SHAP values vs. X_train_tf_df data - same # columns\n",
    "shap_values[1].shape, X_test_tf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.578167Z",
     "start_time": "2021-09-08T21:08:25.167Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Visualizing top 25 importances\n",
    "# shap.summary_plot(shap_values, X_test_tf_df, plot_type='bar', max_display=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.579167Z",
     "start_time": "2021-09-08T21:08:25.172Z"
    }
   },
   "outputs": [],
   "source": [
    "## Better plot\n",
    "shap.summary_plot(shap_values,X_shap,max_display=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Force Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.580167Z",
     "start_time": "2021-09-08T21:08:25.183Z"
    }
   },
   "outputs": [],
   "source": [
    "target_lookup = {0:'Check-Out',1:'Canceled'}\n",
    "target_lookup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.581168Z",
     "start_time": "2021-09-08T21:08:25.187Z"
    }
   },
   "outputs": [],
   "source": [
    "row = np.random.choice(range(len(shap_values)))\n",
    "print(f\"- Row #: {row}\")\n",
    "print(f\"Class = {target_lookup[y_test.iloc[row]]}\")\n",
    "X_test_tf_df.iloc[row].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.582166Z",
     "start_time": "2021-09-08T21:08:25.193Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.584168Z",
     "start_time": "2021-09-08T21:08:25.198Z"
    }
   },
   "outputs": [],
   "source": [
    "## Individual forceplot\n",
    "shap.force_plot(explainer.expected_value, shap_values[1][row],shap_values.iloc[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.585168Z",
     "start_time": "2021-09-08T21:08:25.203Z"
    }
   },
   "outputs": [],
   "source": [
    "## Overall Forceplot\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1],X_test_tf_df)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T21:15:32.587167Z",
     "start_time": "2021-09-08T21:08:25.207Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('Lead Time',shap_values[1],X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CLF results - feature importances\n",
    "* feature importances - visualize via SHAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After testing several models, I found that MODELNAMEHERE produced the most accurate results.\n",
    ">\n",
    "> TOP 5 STRONGEST INDICATORS - Canceled Reservations:\n",
    "> * \n",
    "> * \n",
    "> * \n",
    ">\n",
    "> TOP 5 STRONGEST INDICATORS - Actualized Reservations:\n",
    "> * \n",
    "> * \n",
    "> * \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Operationally, these results give us data-supported insights into our future guests and their needs. Once deployed, hotels would be able to use this model to forecast potential occupancy and staffing/supply needs. \n",
    ">\n",
    "> Additionally, Operations teams would be able to determine how many and which guests would be the most likely to cancel their reservations. This information is very useful during periods of high-occupancy, particularly when trying to determine which guests to relocate in case of an oversold hotel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Time series modeling - forecasting based off of daily average probabilities (for a given # of arrivals, what is the forecast of %/# CXL?\n",
    "\n",
    "> TSM - vector autoregressive forecasting using features to predict # cxl\n",
    "\n",
    "> *Major stretch goal/future work:* determining likelihood of cancellations at given thresholds - e.g. 0-3, 4-7, 7-14, etc. days out\n",
    "* What would be feature importances/coefficients at each threshold?\n",
    "* Could I group the reservations based on their lead time despite different years?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_intel_env]",
   "language": "python",
   "name": "conda-env-test_intel_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.591px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

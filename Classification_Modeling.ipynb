{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cancel Culture - Classification Modeling Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Post-Cleaning Modeling Notebook**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- > ðŸ›‘ **FIX**: Add cmts re: post-cleaning, modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The Basics**\n",
    "\n",
    ">I will import the usual packages: Pandas, Numpy, Matplotlib, and Seaborn. Additionally, I have several personal functions that I use during the modeling process.\n",
    "\n",
    "**More Models**\n",
    "\n",
    "> When I begin the modeling process, I will import \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:33.788705Z",
     "start_time": "2021-09-08T19:38:33.734702Z"
    }
   },
   "outputs": [],
   "source": [
    "## Jupyter Notebook setting to reload functions when called\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:39.372703Z",
     "start_time": "2021-09-08T19:38:33.790714Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Visualizing model results\n",
    "import shap\n",
    "\n",
    "## Personal functions\n",
    "from bmc_functions import classification as clf\n",
    "\n",
    "## SKLearn and Modeling Tools\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,\\\n",
    "                                    RepeatedStratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:39.656699Z",
     "start_time": "2021-09-08T19:38:39.375701Z"
    }
   },
   "outputs": [],
   "source": [
    "## Settings\n",
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.25)\n",
    "\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 50)\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Speeding-Up Scikit-Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Due to the size of my dataset, the modeling process took a fair amount of time, especially when testing different model types. To improve my models' runtime, I use a package called \"**Intel(R) Extension for Scikit-learn*.**\"\n",
    "\n",
    "This package operates in the background to increase the computational efficiency of certain Scikit-Learn models, including Logistic Regression and Random Forest Classifier models. The package does not affect the model results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:45.657702Z",
     "start_time": "2021-09-08T19:38:45.367705Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Speeding up SKLearn via Intel(R) Extension for Scikit-learn*\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inmporting models separately due to \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading the DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In my prior EDA notebook, I reviewed, cleaned, and performed some pre-processing steps to prepare my data separately before modeling. I saved the data as a .pickle file to preserve the datatypes; now I will re-read the data for modeling purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:40.180702Z",
     "start_time": "2021-09-08T19:38:39.659701Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/data_prepped.pickle',\n",
    "                           compression = 'gzip')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:40.418701Z",
     "start_time": "2021-09-08T19:38:40.182704Z"
    }
   },
   "outputs": [],
   "source": [
    "## Identifying target\n",
    "target= 'is_canceled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:40.729699Z",
     "start_time": "2021-09-08T19:38:40.421704Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping target and \"reservation_status\" (nearly identical indicator)\n",
    "\n",
    "X = data.drop(columns = [target, 'reservation_status']).copy()\n",
    "y = data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:41.172701Z",
     "start_time": "2021-09-08T19:38:40.731701Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking for missing values\n",
    "print(f'Missing values for X:\\n {X.isna().sum()[X.isna().sum() >0]}\\n')\n",
    "print(f'Missing values for y: {y.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:41.552701Z",
     "start_time": "2021-09-08T19:38:41.175703Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting - stratify to maintain class balance b/t X_train/_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:41.818702Z",
     "start_time": "2021-09-08T19:38:41.555702Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving memory by deleting unused X, y\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:42.068702Z",
     "start_time": "2021-09-08T19:38:41.820701Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "num_cols = X_train.select_dtypes('number').columns.to_list()\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:42.340700Z",
     "start_time": "2021-09-08T19:38:42.070702Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.to_list()\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepping the Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Pipeline to streamline modeling steps:\n",
    "* Preprocessing: OHE, scaling, outliers via Æ’-XF?\n",
    "* Modeling: RFC, BRFC\n",
    "* GSCV: include as part of pipeline\n",
    "* Get results:\n",
    "    * Feature importances - **SHAP**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:42.610701Z",
     "start_time": "2021-09-08T19:38:42.342701Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "### --- Creating column pipelines --- ###\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown='ignore',\n",
    "                                                 sparse=False))])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "## Instantiating the ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipe, num_cols),\n",
    "                  ('cat', cat_pipe, cat_cols)\n",
    "                  ])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:43.909749Z",
     "start_time": "2021-09-08T19:38:42.612700Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cat'].named_steps['ohe'].get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]\n",
    "\n",
    "final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:45.363701Z",
     "start_time": "2021-09-08T19:38:43.911703Z"
    }
   },
   "outputs": [],
   "source": [
    "## Transform the data via the ColumnTransformer preprocessor\n",
    "\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Due to class imbalance, will attempt to use \"class_weight = balanced\" to correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting Cross-Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To confirm my models' performance, I will use the `RepeatedStratifiedKFold` cross-validation technique with Scikit-Learn's  `cross_val_score()` function to validate each fitted model's performance.\n",
    ">\n",
    "> I set the `cv` variable prior to modeling for the convenience during repeated cross-validations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:45.937699Z",
     "start_time": "2021-09-08T19:38:45.659702Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.5\n",
    "> \n",
    "> Testing balanced accuracy score: 0.51\n",
    "> \n",
    "> * *The training score is smaller by 0.01 points.*\n",
    ">\n",
    "> Training data log loss: 16.06\n",
    ">\n",
    "> Testing data log loss: 15.89\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T19:38:47.641704Z",
     "start_time": "2021-09-08T19:38:45.939702Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "\n",
    "base.fit(X_train_tf_df, y_train)\n",
    "\n",
    "clf.evaluate_classification(base,X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test, \n",
    "                           metric = 'balanced accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.82\n",
    "> \n",
    "> Testing balanced accuracy score: 0.82\n",
    "> \n",
    "> * *The scores are the same size.*\n",
    ">\n",
    "> Training data log loss: 0.37\n",
    ">\n",
    "> Testing data log loss: 0.37\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.786Z"
    }
   },
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "logreg = LogisticRegression(max_iter = 600,C = .1,solver = 'lbfgs',\n",
    "                            random_state = 42, n_jobs = 4)\n",
    "\n",
    "logreg.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.788Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(logreg, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.793Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing cross-validation to confirm results\n",
    "\n",
    "accuracy = cross_val_score(logreg, X_train_tf_df, y_train,\n",
    "                           scoring='balanced_accuracy', cv = cv, n_jobs = 4)\n",
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collecting Coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T14:26:58.848264Z",
     "start_time": "2021-09-08T14:26:58.725265Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "> Based on the consistent cross-validation scores, I feel confident in my model's balanced accuracy score. Now I will collect the results for my features and generate a visualization of the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.797Z"
    }
   },
   "outputs": [],
   "source": [
    "## Collecting coefficients for each feature as a Series\n",
    "lr_coefs = pd.Series(logreg.coef_.flatten(), index=X_train_tf_df.columns)\n",
    "lr_coefs.sort_values(ascending=False, inplace=True)\n",
    "lr_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.799Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting top/bottom 5 values into a Series\n",
    "log_odds = pd.concat([lr_coefs.head(5), lr_coefs.tail(5)])\n",
    "log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.802Z"
    }
   },
   "outputs": [],
   "source": [
    "## Formatting index labels to become visualization labels\n",
    "new_labels_list = [i.replace('_', ' ').title() for i in list(log_odds.index)]\n",
    "new_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.805Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating a dictionary to replace the old lables with the new ones\n",
    "new_labels_dict = { k:v for (k,v) in zip(log_odds.index, new_labels_list)}\n",
    "new_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.808Z"
    }
   },
   "outputs": [],
   "source": [
    "## Renaming Series index\n",
    "log_odds = log_odds.rename(new_labels_dict)\n",
    "log_odds.sort_values(inplace=True)\n",
    "\n",
    "log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.811Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Visualizing log-odds\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "ax = log_odds.plot(kind='barh', ax=ax)\n",
    "ax.axvline(linestyle = '-', c='k')\n",
    "ax.set_xlabel('Log-Odds')\n",
    "ax.set_ylabel('Feature Name')\n",
    "fig.suptitle('Top and Bottom Five Features')\n",
    "ax.set_facecolor('0.9')\n",
    "fig.set_facecolor('0.975')\n",
    "# plt.savefig('./img/log_odds.png',transparent=False, bbox_inches='tight',\n",
    "#            dpi=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***May the (Log-)Odds be Ever in Your Favor***\n",
    "\n",
    "> Based on the logistic regression model coefficients, I see that reservations are **most likely to cancel** if they:\n",
    "* Require non-refundable deposits (possibly 3rd-party booking sites like )\n",
    "* Are booked by agents 17 or 240\n",
    "* Have previous cancellations\n",
    "* Guests are from Portugal ([PRT is the three letter ISO 3166-1 code for Portugal](https://en.wikipedia.org/wiki/Portugal#:~:text=ISO%203166%20code,PT))\n",
    ">\n",
    "> Alternatively, reservations are **least likely to cancel** if they:\n",
    "* Do NOT require a deposit\n",
    "* Require parking spaces\n",
    "* Reserve room type \"A\"\n",
    "* Are assigned to room type \"I\"\n",
    "* Are booked by agent 152\n",
    "\n",
    "***Oddities in the Results***\n",
    "\n",
    "> **Non-Refundable Deposit Requirement**\n",
    "* May be associated with 3rd party travel groups like Priceline/Expedia/etc.\n",
    "    * Often require pre-payment to the booking group/agent to confirm booking\n",
    ">\n",
    "> **Country of Origin: Portugal**\n",
    "* Could these hotels be located in Portugal and have a larger percentage of domestic travelers?\n",
    ">\n",
    "> **Room Assignment, Required Car Parking Spaces**\n",
    "* These features may be generated post-stay and may not be available prior to arrival\n",
    "* Parking spot requirements may be specified prior to arrival, but not as likely in my personal experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 0.99\n",
    "> \n",
    "> Testing balanced recall score: 0.88\n",
    ">\n",
    "> * *The training score is larger by 0.11 points.*\n",
    ">\n",
    "> Training data log loss: 0.08\n",
    ">\n",
    "> Testing data log loss: 0.27\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.817Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight = 'balanced',random_state=42,\n",
    "                             n_jobs = 4)\n",
    "\n",
    "rfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.820Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(rfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.824Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(rfc, X_train_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.826Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing cross-validation to confirm results\n",
    "\n",
    "accuracy = cross_val_score(rfc, X_train_tf_df, y_train,\n",
    "                           scoring='balanced_accuracy', cv = cv, n_jobs = 4)\n",
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ExtraTreesClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 1.0\n",
    "> \n",
    "> Testing balanced recall score: 0.87\n",
    "> \n",
    "> * \n",
    ">\n",
    "> Training data log loss: 0.01\n",
    ">\n",
    "> Testing data log loss: 0.33\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.831Z"
    }
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(class_weight ='balanced',random_state=42,\n",
    "                           n_jobs = 4)\n",
    "\n",
    "etc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.834Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(etc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Balanced Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**MODEL: BalancedRandomForestClassifier**\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 0.97\n",
    "> \n",
    "> Testing balanced accuracy score: 0.89\n",
    "> \n",
    "> * *The training score is larger by 0.8 points.*\n",
    ">\n",
    "> Training data log loss: 0.17\n",
    ">\n",
    "> Testing data log loss: 0.30\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> \n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.839Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brfc = BalancedRandomForestClassifier(random_state=42, n_jobs = 4)\n",
    "\n",
    "brfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.842Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(brfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.845Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing cross-validation to confirm results\n",
    "accuracy = cross_val_score(brfc, X_train_tf_df, y_train,\n",
    "                           scoring='balanced_accuracy', cv = cv, n_jobs = 4)\n",
    "accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interpreting Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›‘ FIX: adjust for context of only B/RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Odd Features**\n",
    "\n",
    "> Now that I completed my modeling steps, I will review the results of each model and determine my final recommendations.\n",
    ">\n",
    "> My main models are a standard logistic regression and a Balanced Random Forest classifier (\"BRFC\"). **Each model provides a different way of identifying which features are most impactful: logistic regressions provide \"log-odds\" and a Balanced Random Forest Classifier produces \"feature importances.\"** Both will require some processing for easy interpretation.\n",
    "\n",
    "**Feature Importances and SHAP**\n",
    "\n",
    "> As I mention above, tree-based models, including my BRFC-model, return \"feature importances\" instead of the coefficients associated with linear/logistic regressions. These values are useful to show the impact of a given feature on the decision-making steps of the tree model. \n",
    ">\n",
    ">However, these feature importances suffer from one key weakness: *they do not indicate if a feature increases or decreases the likelihood of a reservation canceling (my target feature).*\n",
    ">\n",
    "> I will utilize a visualization package called **SHAP** to produce \"Shapely values\" for each feature. These values indicate each feature's marginal contribution to the model - answering the question, \"*How well does the model perform with this feature than without?*\" \n",
    "\n",
    "**Seeing is Believing**\n",
    "\n",
    ">Using tools within the package, I will focus on two visualizations:\n",
    "> * The `summary_plot`: visualizing each feature's Shapely value and the feature's values from low-high (relative to each feature).\n",
    ">\n",
    ">\n",
    "> * The `force_plot`: an in-depth look at the forces impacting any given reservation record.\n",
    ">\n",
    "> More information about SHAP:\n",
    "* [SHAP Documentation](https://shap.readthedocs.io/en/latest/?badge=latest)\n",
    "* [SHAP Repository](https://github.com/slundberg/shap)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "> \n",
    "\n",
    "**Process**\n",
    "\n",
    "> \n",
    "\n",
    "**Performance**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›‘ Fix: Annotate Code and Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.852Z"
    }
   },
   "outputs": [],
   "source": [
    " ## Initializing Javascript for SHAP models\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:38:33.855Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generating a sample of the overall data for review:\n",
    "\n",
    "X_shap = shap.sample(X_test_tf_df, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T19:35:46.811Z"
    }
   },
   "outputs": [],
   "source": [
    "## Initializing an explainer with the RandomForestClassifier model\n",
    "t_explainer = shap.TreeExplainer(rfc)\n",
    "\n",
    "## Calculating SHAP values for test data\n",
    "shap_values = t_explainer.shap_values(X_shap,y_test)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_explainer = shap.KernelExplainer(rfc.predict, X_shap)\n",
    "k_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T18:51:19.703Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate shap values for test data\n",
    "shap_values = explainer.shap_values(X_test_tf_df)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T16:08:28.687093Z",
     "start_time": "2021-09-08T16:08:28.445093Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting sizes of SHAP values vs. X_train_tf_df data - same # columns\n",
    "shap_values.shape, X_test_tf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T16:10:40.569845Z",
     "start_time": "2021-09-08T16:10:39.980845Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Visualizing top 25 importances\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_tf_df, plot_type='bar', max_display=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T16:46:00.111219Z",
     "start_time": "2021-09-08T16:45:58.777222Z"
    }
   },
   "outputs": [],
   "source": [
    "## Better plot\n",
    "\n",
    "shap.summary_plot(shap_values,X_shap,max_display=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Force Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T16:46:27.559841Z",
     "start_time": "2021-09-08T16:46:27.310815Z"
    }
   },
   "outputs": [],
   "source": [
    "target_lookup = {0:'Check-Out',1:'Canceled'}\n",
    "target_lookup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:09:39.148568Z",
     "start_time": "2021-09-08T17:09:38.867569Z"
    }
   },
   "outputs": [],
   "source": [
    "row = np.random.choice(range(len(shap_values)))\n",
    "print(f\"- Row #: {row}\")\n",
    "print(f\"Class = {target_lookup[y_test.iloc[row]]}\")\n",
    "X_test_tf_df.iloc[row].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:09:42.972577Z",
     "start_time": "2021-09-08T17:09:42.688574Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:09:44.455572Z",
     "start_time": "2021-09-08T17:09:44.146576Z"
    }
   },
   "outputs": [],
   "source": [
    "## Individual forceplot\n",
    "shap.force_plot(explainer.expected_value, shap_values[1][row],shap_values.iloc[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T13:17:00.295Z"
    }
   },
   "outputs": [],
   "source": [
    "## Overall Forceplot\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1],X_test_tf_df)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-08T13:17:00.300Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot('Lead Time',shap_values[1],X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CLF results - feature importances\n",
    "* feature importances - visualize via SHAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After testing several models, I found that MODELNAMEHERE produced the most accurate results.\n",
    ">\n",
    "> TOP 5 STRONGEST INDICATORS - Canceled Reservations:\n",
    "> * \n",
    "> * \n",
    "> * \n",
    ">\n",
    "> TOP 5 STRONGEST INDICATORS - Actualized Reservations:\n",
    "> * \n",
    "> * \n",
    "> * \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Operationally, these results give us data-supported insights into our future guests and their needs. Once deployed, hotels would be able to use this model to forecast potential occupancy and staffing/supply needs. \n",
    ">\n",
    "> Additionally, Operations teams would be able to determine how many and which guests would be the most likely to cancel their reservations. This information is very useful during periods of high-occupancy, particularly when trying to determine which guests to relocate in case of an oversold hotel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Time series modeling - forecasting based off of daily average probabilities (for a given # of arrivals, what is the forecast of %/# CXL?\n",
    "\n",
    "> TSM - vector autoregressive forecasting using features to predict # cxl\n",
    "\n",
    "> *Major stretch goal/future work:* determining likelihood of cancellations at given thresholds - e.g. 0-3, 4-7, 7-14, etc. days out\n",
    "* What would be feature importances/coefficients at each threshold?\n",
    "* Could I group the reservations based on their lead time despite different years?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_intel_env]",
   "language": "python",
   "name": "conda-env-test_intel_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.591px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

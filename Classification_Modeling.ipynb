{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cancel Culture - Classification Modeling Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Post-Cleaning Modeling Notebook**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- > ðŸ›‘ **FIX**: Add cmts re: post-cleaning, modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> I will import several packages and modules to perform my modeling steps. I use different parts of Scikit-Learn's numerous packages for the majority of my modeling. In addition to SKLearn, I tested models from XGBoost as well as Imblearn towards the end of the modeling process.\n",
    ">\n",
    "> I include the usual packages as well: pandas, numpy, matplotlib, and seaborn. Additionally, I have several personal functions that I use during the modeling process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:36:01.245975Z",
     "start_time": "2021-09-08T00:36:01.185964Z"
    }
   },
   "outputs": [],
   "source": [
    "## Jupyter Notebook setting to reload functions when called\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:36:05.573747Z",
     "start_time": "2021-09-08T00:36:01.248966Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Personal functions\n",
    "from bmc_functions import classification as clf\n",
    "\n",
    "## SKLearn and Modeling Tools\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:44:10.198023Z",
     "start_time": "2021-09-08T00:44:10.067924Z"
    }
   },
   "outputs": [],
   "source": [
    "## Settings\n",
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.25)\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading the DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In my prior EDA notebook, I reviewed, cleaned, and performed some pre-processing steps to prepare my data separately before modeling. I saved the data as a .pickle file to preserve the datatypes; now I will re-read the data for modeling purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:43.135985Z",
     "start_time": "2021-09-08T00:38:42.739983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/data_prepped.pickle',\n",
    "                           compression = 'gzip')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:43.307981Z",
     "start_time": "2021-09-08T00:38:43.138988Z"
    }
   },
   "outputs": [],
   "source": [
    "## Identifying target\n",
    "target= 'is_canceled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:43.477982Z",
     "start_time": "2021-09-08T00:38:43.309983Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping target and \"reservation_status\" (nearly identical indicator)\n",
    "\n",
    "X = data.drop(columns = [target, 'reservation_status']).copy()\n",
    "y = data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:43.757983Z",
     "start_time": "2021-09-08T00:38:43.480984Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking for missing values\n",
    "print(f'Missing values for X:\\n {X.isna().sum()[X.isna().sum() >0]}\\n')\n",
    "print(f'Missing values for y: {y.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:44.009982Z",
     "start_time": "2021-09-08T00:38:43.759983Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting - stratify to maintain class balance b/t X_train/_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:44.149982Z",
     "start_time": "2021-09-08T00:38:44.012985Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving memory by deleting unused X, y\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:44.290982Z",
     "start_time": "2021-09-08T00:38:44.152987Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "num_cols = X_train.select_dtypes('number').columns.to_list()\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:44.445981Z",
     "start_time": "2021-09-08T00:38:44.293983Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.to_list()\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepping the Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Pipeline to streamline modeling steps:\n",
    "* Preprocessing: OHE, scaling, outliers via Æ’-XF?\n",
    "* Modeling: RFC, BRFC\n",
    "* GSCV: include as part of pipeline\n",
    "* Get results:\n",
    "    * Feature importances - **SHAP**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:44.633997Z",
     "start_time": "2021-09-08T00:38:44.499985Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "### --- Creating column pipelines --- ###\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown='ignore',\n",
    "                                                 sparse=False))])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "## Instantiating the ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipe, num_cols),\n",
    "                  ('cat', cat_pipe, cat_cols)\n",
    "                  ])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:45.831983Z",
     "start_time": "2021-09-08T00:38:44.686984Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cat'].named_steps['ohe'].get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]\n",
    "\n",
    "final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:47.233997Z",
     "start_time": "2021-09-08T00:38:45.834987Z"
    }
   },
   "outputs": [],
   "source": [
    "## Transform the data via the ColumnTransformer preprocessor\n",
    "\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Due to class imbalance, will attempt to use \"class_weight = balanced\" to correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Speeding-Up Scikit-Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Due to the size of my dataset, the modeling process took a fair amount of time, especially when testing different model types. To improve my models' runtime, I use a package called \"**Intel(R) Extension for Scikit-learn*.**\"\n",
    "\n",
    "This package operates in the background to increase the computational efficiency of certain Scikit-Learn models, including Logistic Regression and Random Forest Classifier models. The package does not affect the model results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:38:48.756017Z",
     "start_time": "2021-09-08T00:38:48.631988Z"
    }
   },
   "outputs": [],
   "source": [
    "## Speeding up SKLearn via Intel(R) Extension for Scikit-learn*\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:44:20.651259Z",
     "start_time": "2021-09-08T00:44:19.239260Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "\n",
    "base.fit(X_train_tf_df, y_train)\n",
    "\n",
    "clf.evaluate_classification(base,X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test, \n",
    "                           metric = 'balanced accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.82\n",
    "> \n",
    "> Testing balanced accuracy score: 0.82\n",
    "> \n",
    "> * *The scores are the same size.*\n",
    ">\n",
    "> Training data log loss: 0.37\n",
    ">\n",
    "> Testing data log loss: 0.37\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:39:04.115002Z",
     "start_time": "2021-09-08T00:38:48.758016Z"
    }
   },
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter = 600,C = .1,solver = 'lbfgs',\n",
    "                            n_jobs=-1,random_state = 42)\n",
    "\n",
    "logreg.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:44:28.453612Z",
     "start_time": "2021-09-08T00:44:26.711612Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(logreg, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> I feel that this regression model is showing strong results, so I will collect the results for my features and generate a visualization of the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:39:05.898142Z",
     "start_time": "2021-09-08T00:39:05.775003Z"
    }
   },
   "outputs": [],
   "source": [
    "## Collecting coefficients for each feature as a Series\n",
    "lr_coefs = pd.Series(logreg.coef_.flatten(), index=X_train_tf_df.columns)\n",
    "lr_coefs.sort_values(ascending=False, inplace=True)\n",
    "lr_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:39:06.022994Z",
     "start_time": "2021-09-08T00:39:05.901025Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting top/bottom 5 values into a Series\n",
    "log_odds = pd.concat([lr_coefs.head(5), lr_coefs.tail(5)])\n",
    "log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:39:06.164995Z",
     "start_time": "2021-09-08T00:39:06.026002Z"
    }
   },
   "outputs": [],
   "source": [
    "## Formatting index labels to become visualization labels\n",
    "new_labels_list = [i.replace('_', ' ').title() for i in list(log_odds.index)]\n",
    "new_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:39:06.291098Z",
     "start_time": "2021-09-08T00:39:06.168009Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating a dictionary to replace the old lables with the new ones\n",
    "new_labels_dict = { k:v for (k,v) in zip(log_odds.index, new_labels_list)}\n",
    "new_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T00:39:06.400996Z",
     "start_time": "2021-09-08T00:39:06.293011Z"
    }
   },
   "outputs": [],
   "source": [
    "## Renaming Series index\n",
    "log_odds = log_odds.rename(new_labels_dict)\n",
    "log_odds.sort_values(inplace=True)\n",
    "\n",
    "log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T01:41:12.545567Z",
     "start_time": "2021-09-08T01:41:12.085595Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Visualizing log-odds\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "ax = log_odds.plot(kind='barh', ax=ax)\n",
    "ax.axvline(linestyle = '-', c='k')\n",
    "ax.set_xlabel('Log-Odds')\n",
    "ax.set_ylabel('Feature Name')\n",
    "ax.set_title('Top and Bottom Five Features')\n",
    "ax.set_facecolor('0.9')\n",
    "fig.set_facecolor('0.975')\n",
    "# plt.savefig('./img/log_odds.png',transparent=False, bbox_inches='tight',\n",
    "#            dpi=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "***May the (Log-)Odds be Ever in Your Favor***\n",
    "\n",
    "> Based on the logistic regression model coefficients, I see that reservations are **most likely to cancel** if they:\n",
    "* Require non-refundable deposits (possibly 3rd-party booking sites like )\n",
    "* Are booked by agents 17 or 240\n",
    "* Have previous cancellations\n",
    "* Guests are from Portugal ([PRT is the three letter ISO 3166-1 code for Portugal](https://en.wikipedia.org/wiki/Portugal#:~:text=ISO%203166%20code,PT))\n",
    ">\n",
    "> Alternatively, reservations are **least likely to cancel** if they:\n",
    "* Do NOT require a deposit\n",
    "* Require parking spaces\n",
    "* Reserve room type \"A\"\n",
    "* Are assigned to room type \"I\"\n",
    "* Are booked by agent 152\n",
    "\n",
    "***Oddities in the Results***\n",
    "\n",
    "> **Non-Refundable Deposit Requirement**\n",
    "* May be associated with 3rd party travel groups like Priceline/Expedia/etc.\n",
    "    * Often require pre-payment to the booking group/agent to confirm booking\n",
    ">\n",
    "> **Country of Origin: Portugal**\n",
    "* Could these hotels be located in Portugal and have a larger percentage of domestic travelers?\n",
    ">\n",
    "> **Room Assignment, Required Car Parking Spaces**\n",
    "* These features may be generated post-stay and may not be available prior to arrival\n",
    "* Parking spot requirements may be specified prior to arrival, but not as likely in my personal experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GridSearchCV - Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> *Results unavailable - did not run model due to time required*\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:33.443564Z",
     "start_time": "2021-09-07T19:27:33.181562Z"
    }
   },
   "outputs": [],
   "source": [
    "# lr_params = {\n",
    "#  'C': [.001, .01, .1, 1],\n",
    "#     'penalty':['l1', 'l2'],#, 'elasticnet', 'none'],\n",
    "#     'solver':['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs'],\n",
    "#     'max_iter':[100, 300, 500]}\n",
    "\n",
    "# gscv = GridSearchCV(LogisticRegression(class_weight='balanced'), lr_params,\n",
    "#                     scoring = 'balanced_accuracy', cv=3, n_jobs = -1)\n",
    "# gscv\n",
    "\n",
    "# gscv.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:33.679566Z",
     "start_time": "2021-09-07T19:27:33.446562Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf.evaluate_classification(gscv.best_estimator_, X_train = X_train_tf_df, y_train = y_train,\n",
    "#                            X_test = X_test_tf_df, y_test = y_test,\n",
    "#                           metric = 'balanced accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:33.884564Z",
     "start_time": "2021-09-07T19:27:33.683571Z"
    }
   },
   "outputs": [],
   "source": [
    "# logreg_params = gscv.best_params_\n",
    "\n",
    "# logreg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:34.087572Z",
     "start_time": "2021-09-07T19:27:33.887567Z"
    }
   },
   "outputs": [],
   "source": [
    "# gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 0.99\n",
    "> \n",
    "> Testing balanced recall score: 0.88\n",
    ">\n",
    "> * *The training score is larger by 0.11 points.*\n",
    ">\n",
    "> Training data log loss: 0.08\n",
    ">\n",
    "> Testing data log loss: 0.27\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:17.325354Z",
     "start_time": "2021-09-07T19:26:35.397810Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(class_weight = 'balanced', n_jobs=-1,\n",
    "                             random_state=42)\n",
    "\n",
    "rfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:31.221535Z",
     "start_time": "2021-09-07T19:27:17.328351Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(rfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:35.925594Z",
     "start_time": "2021-09-07T19:27:35.360584Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(rfc, X_train_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GSCV - RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 0.99\n",
    "> \n",
    "> Testing balanced accuracy score: 0.88\n",
    "> \n",
    "> * *The training score is larger by 0.11 points.*\n",
    ">\n",
    "> Training data log loss: 0.10\n",
    ">\n",
    "> Testing data log loss: 0.26\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> 'criterion': 'entropy'\n",
    "> \n",
    "> 'max_depth': None\n",
    "> \n",
    "> 'min_samples_leaf': 2\n",
    "> \n",
    "> 'min_samples_split': 2\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:34.289574Z",
     "start_time": "2021-09-07T19:27:34.090574Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_params = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [25,50, None],\n",
    "#     'min_samples_split': [2,3,4],\n",
    "#     'min_samples_leaf': [1,2]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:34.513575Z",
     "start_time": "2021-09-07T19:27:34.292579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rfgs = GridSearchCV(RandomForestClassifier(class_weight = 'balanced',\n",
    "#                                            random_state = 42, n_jobs=-1),\n",
    "#                     rfc_params,scoring = 'balanced_accuracy',\n",
    "#                     cv=3,verbose = 4)\n",
    "\n",
    "# rfgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:34.733583Z",
     "start_time": "2021-09-07T19:27:34.517576Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:34.919578Z",
     "start_time": "2021-09-07T19:27:34.737578Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:35.140587Z",
     "start_time": "2021-09-07T19:27:34.922580Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_new = rfgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:35.355589Z",
     "start_time": "2021-09-07T19:27:35.143586Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf.evaluate_classification(rfc_new, X_train_tf_df, y_train, X_test_tf_df, \n",
    "#                            y_test, 'balanced recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ExtraTreesClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 1.0\n",
    "> \n",
    "> Testing balanced recall score: 0.87\n",
    "> \n",
    "> * \n",
    ">\n",
    "> Training data log loss: 0.01\n",
    ">\n",
    "> Testing data log loss: 0.33\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:32.492551Z",
     "start_time": "2021-09-07T19:27:32.289548Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# etc = ExtraTreesClassifier(class_weight = 'balanced', n_jobs=-1, random_state=42)\n",
    "\n",
    "# etc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:27:32.693552Z",
     "start_time": "2021-09-07T19:27:32.496553Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf.evaluate_classification(etc, X_train = X_train_tf_df, y_train = y_train,\n",
    "#                            X_test = X_test_tf_df, y_test = y_test,\n",
    "#                           metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Balanced Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**MODEL: BalancedRandomForestClassifier**\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 0.97\n",
    "> \n",
    "> Testing balanced accuracy score: 0.89\n",
    "> \n",
    "> * *The training score is larger by 0.8 points.*\n",
    ">\n",
    "> Training data log loss: 0.17\n",
    ">\n",
    "> Testing data log loss: 0.30\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> \n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T19:17:56.380163Z",
     "start_time": "2021-09-07T19:17:56.365143Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "balanced_forest = BalancedRandomForestClassifier(n_jobs = -1, random_state=42)\n",
    "\n",
    "balanced_forest.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:31:02.853000Z",
     "start_time": "2021-09-07T02:30:55.068968Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(balanced_forest, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GSCV: Balanced Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**MODEL:** GridSearchCV - BalancedBaggingClassifier\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 0.96\n",
    "> \n",
    "> Testing balanced accuracy score: 0.89\n",
    "> \n",
    "> * *The training score is larger by 0.07 points.*\n",
    ">\n",
    "> Training data log loss: 0.20\n",
    ">\n",
    "> Testing data log loss: 0.30\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> * 'criterion': 'entropy'\n",
    ">\n",
    "> * 'max_features': None\n",
    ">\n",
    "> * 'min_samples_leaf': 1\n",
    ">\n",
    "> * 'min_samples_split': 2\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:32:22.759398Z",
     "start_time": "2021-09-07T02:32:22.647310Z"
    }
   },
   "outputs": [],
   "source": [
    "brfc_params = {'criterion': ['gini', 'entropy'],\n",
    "               'min_samples_split': [2, 3, 4],\n",
    "               'min_samples_leaf': [1,2,3]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:43:48.367824Z",
     "start_time": "2021-09-07T02:32:22.790304Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brfc = GridSearchCV(BalancedRandomForestClassifier(n_jobs=-1,random_state=42), \n",
    "                    brfc_params,scoring = 'balanced_accuracy',\n",
    "                    cv=3,verbose = 4)\n",
    "\n",
    "brfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:48:31.173866Z",
     "start_time": "2021-09-07T02:48:23.143901Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(brfc, X_train_tf_df, y_train, X_test_tf_df, \n",
    "                           y_test, 'balanced precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:46:13.542457Z",
     "start_time": "2021-09-07T02:46:13.416458Z"
    }
   },
   "outputs": [],
   "source": [
    "brfc_params = brfc.best_params_\n",
    "\n",
    "brfc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:47:40.839489Z",
     "start_time": "2021-09-07T02:47:40.711456Z"
    }
   },
   "outputs": [],
   "source": [
    "brfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:52:44.914376Z",
     "start_time": "2021-09-07T02:52:44.799715Z"
    }
   },
   "outputs": [],
   "source": [
    "brfc_params = {'criterion': ['entropy'],\n",
    "               'min_samples_split': [2],\n",
    "               'min_samples_leaf': [1],\n",
    "               'class_weight': ['balanced', 'balanced_subsample']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:54:31.378132Z",
     "start_time": "2021-09-07T02:52:45.560309Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brfc = GridSearchCV(BalancedRandomForestClassifier(n_jobs=-1,random_state=42), \n",
    "                    brfc_params,scoring = 'balanced_accuracy',\n",
    "                    cv=3,verbose = 4)\n",
    "\n",
    "brfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T02:54:39.496954Z",
     "start_time": "2021-09-07T02:54:31.381136Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(brfc, X_train_tf_df, y_train, X_test_tf_df, \n",
    "                           y_test, 'balanced precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interpretation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Odd Features**\n",
    "\n",
    "> Now that I completed my modeling steps, I will review the results of each model and determine my final recommendations.\n",
    ">\n",
    "> My main models are a standard logistic regression and a Balanced Random Forest classifier (\"BRFC\"). **Each model provides a different way of identifying which features are most impactful: logistic regressions provide \"log-odds\" and a Balanced Random Forest Classifier produces \"feature importances.\"** Both will require some processing for easy interpretation.\n",
    "\n",
    "**Feature Importances and SHAP**\n",
    "\n",
    "> As I mention above, tree-based models, including my BRFC-model, return \"feature importances\" instead of the coefficients associated with linear/logistic regressions. These values are useful to show the impact of a given feature on the decision-making steps of the tree model. \n",
    ">\n",
    ">However, these feature importances suffer from one key weakness: *they do not indicate if a feature increases or decreases the likelihood of a reservation canceling (my target feature).*\n",
    ">\n",
    "> I will utilize a visualization package called **SHAP** to produce \"Shapely values\" for each feature. These values indicate each feature's marginal contribution to the model - answering the question, \"*How well does the model perform with this feature than without?*\" \n",
    "\n",
    "**Seeing is Believing**\n",
    "\n",
    ">Using tools within the package, I will focus on two visualizations:\n",
    "> * The `summary_plot`: visualizing each feature's Shapely value and the feature's values from low-high (relative to each feature).\n",
    ">\n",
    ">\n",
    "> * The `force_plot`: an in-depth look at the forces impacting any given reservation record.\n",
    ">\n",
    "> More information about SHAP:\n",
    "* [SHAP Documentation](https://shap.readthedocs.io/en/latest/?badge=latest)\n",
    "* [SHAP Repository](https://github.com/slundberg/shap)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "> \n",
    "\n",
    "**Process**\n",
    "\n",
    "> \n",
    "\n",
    "**Performance**\n",
    "\n",
    "> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›‘ Fix: Annotate Code and Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T02:07:32.261952Z",
     "start_time": "2021-09-08T02:07:32.055983Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap \n",
    "print(shap.__version__)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May need to gather sub-sample of data, ex:\n",
    "\n",
    "X_shap = shap.sample(X_test_tf_df,nsamples=200)\n",
    "explainer = shap.TreeExplainer(brfc.predict,X_shap)\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Initialize an explainer with the model\n",
    "# explainer = shap.TreeExplainer(rf)\n",
    "\n",
    "## Calculaate shap values for test data\n",
    "shap_values = explainer.shap_values(X_test_tf_df,y_test)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[1].shape, X_test_tf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bar\n",
    "\n",
    "shap.summary_plot(shap_values[1], X_test_tf_df,plot_type='bar',max_display=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better plot\n",
    "\n",
    "shap.summary_plot(shap_values[1],X_test_tf_df,max_display=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Force Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T02:09:15.662011Z",
     "start_time": "2021-09-08T02:09:15.638042Z"
    }
   },
   "outputs": [],
   "source": [
    "target_lookup = {0:'Check-Out',1:'Canceled'}\n",
    "target_lookup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = np.random.choice(range(len(X_test_tf_df)))\n",
    "print(f\"- Row #: {row}\")\n",
    "print(f\"Class = {target_lookup[y_test.iloc[row]]}\")\n",
    "X_test_tf_df.iloc[row].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Individual forceplot\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][row],X_test_tf_df.iloc[row])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall Forceplot\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1],X_test_tf_df)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Lead Time',shap_values[1],X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CLF results - feature importances\n",
    "* feature importances - visualize via SHAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After testing several models, I found that MODELNAMEHERE produced the most accurate results.\n",
    ">\n",
    "> TOP 5 STRONGEST INDICATORS - Canceled Reservations:\n",
    "> * \n",
    "> * \n",
    "> * \n",
    ">\n",
    "> TOP 5 STRONGEST INDICATORS - Actualized Reservations:\n",
    "> * \n",
    "> * \n",
    "> * \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Operationally, these results give us data-supported insights into our future guests and their needs. Once deployed, hotels would be able to use this model to forecast potential occupancy and staffing/supply needs. \n",
    ">\n",
    "> Additionally, Operations teams would be able to determine how many and which guests would be the most likely to cancel their reservations. This information is very useful during periods of high-occupancy, particularly when trying to determine which guests to relocate in case of an oversold hotel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Time series modeling - forecasting based off of daily average probabilities (for a given # of arrivals, what is the forecast of %/# CXL?\n",
    "\n",
    "> TSM - vector autoregressive forecasting using features to predict # cxl\n",
    "\n",
    "> *Major stretch goal/future work:* determining likelihood of cancellations at given thresholds - e.g. 0-3, 4-7, 7-14, etc. days out\n",
    "* What would be feature importances/coefficients at each threshold?\n",
    "* Could I group the reservations based on their lead time despite different years?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_intel_env]",
   "language": "python",
   "name": "conda-env-test_intel_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

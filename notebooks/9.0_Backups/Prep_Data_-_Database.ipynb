{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quack Quack - Creating the DuckDB\n",
    "\n",
    "---\n",
    "\n",
    "**Ducking the Data with a Database**\n",
    "\n",
    "As this project will involve many data transformations; engineered features; and iterative modeling; I need an orderly, robust system to handle all of the data, models, etc. without creating too much complexity in the repository. Instead of creating separate files for each version of the data, I decided that I need to create a small database to store the information  effectively and efficiently. Before I can create the database, I need data!\n",
    "\n",
    "**Hatching the Plan**\n",
    "\n",
    "I obtained my source data from the article referenced in the `README.md` file. The source data comes in the form of two separate CSV files, which are both sizeable and take a while to load into a dataframe (or in this case, a database). To reduce the size and increase read/write times, I will convert the original source files from CSVs to parquet files. Then, I will take the raw reservation data and use it to create the first table within the database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:04:29.548799Z",
     "iopub.status.busy": "2024-05-09T11:04:29.547799Z",
     "iopub.status.idle": "2024-05-09T11:04:29.563817Z",
     "shell.execute_reply": "2024-05-09T11:04:29.562798Z",
     "shell.execute_reply.started": "2024-05-09T11:04:29.548799Z"
    }
   },
   "outputs": [],
   "source": [
    "## Enabling access to custom functions in separate directory\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Construct the absolute path to the 'src' directory\n",
    "src_path = os.path.abspath(os.path.join('../../', 'src'))\n",
    "\n",
    "# Append the path to 'sys.path'\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:05:22.945323Z",
     "iopub.status.busy": "2024-05-09T11:05:22.944331Z",
     "iopub.status.idle": "2024-05-09T11:05:22.962325Z",
     "shell.execute_reply": "2024-05-09T11:05:22.961330Z",
     "shell.execute_reply.started": "2024-05-09T11:05:22.945323Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Source CSVs to Parquet\n",
    "\n",
    "---\n",
    "\n",
    "The following code loops through this repository's `/data/` directory; searches for the source CSVs; converts each of them to a parquet file; and then deletes the CSV.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1\n",
      "H2\n"
     ]
    }
   ],
   "source": [
    "# # Define the directory containing the CSV files\n",
    "# path = Path('../../data/source/')\n",
    "\n",
    "# for file in path.glob('*.csv'):\n",
    "#     print(str(file)[-6:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:01:41.612411Z",
     "iopub.status.busy": "2024-05-09T11:01:41.611413Z",
     "iopub.status.idle": "2024-05-09T11:01:41.623416Z",
     "shell.execute_reply": "2024-05-09T11:01:41.622407Z",
     "shell.execute_reply.started": "2024-05-09T11:01:41.612411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed.\n"
     ]
    }
   ],
   "source": [
    "# # Define the directory containing the CSV files\n",
    "# path = Path('../data/source/')\n",
    "\n",
    "# for file in path.glob('*.csv'):\n",
    "#     try:\n",
    "#         # Read the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(file)\n",
    "        \n",
    "#         df['HotelNumber'] = str(file)[-6:-4]\n",
    "        \n",
    "#         # Define the Parquet file path (same name as the CSV file but with .parquet extension)\n",
    "#         parquet_file = file.replace('.csv', '.parquet')\n",
    "        \n",
    "#         # Convert the DataFrame to a Parquet file\n",
    "#         df.to_parquet(parquet_file)\n",
    "#         print(f\"Successfully converted {file} to Parquet.\")\n",
    "\n",
    "#         # # If the conversion was successful, remove the CSV file\n",
    "#         # os.remove(file)\n",
    "#         # print(f\"Successfully converted and removed {file}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error converting {file}: {e}\")\n",
    "\n",
    "# print(\"Conversion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Append UUIDs\n",
    "\n",
    "Since the source data was anonymized, there are no unique identifiers for each reservation. To support database joins and relationships between tables, I will add columns for both a UUID and the source hotel number to differentiate the reservations and preserve the unique details of each hotel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:04:32.077571Z",
     "iopub.status.busy": "2024-05-09T11:04:32.076578Z",
     "iopub.status.idle": "2024-05-09T11:04:33.161563Z",
     "shell.execute_reply": "2024-05-09T11:04:33.160571Z",
     "shell.execute_reply.started": "2024-05-09T11:04:32.077571Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_files = ['../data/source/H1.parquet', '../data/source/H2.parquet']\n",
    "# output_files = ['../data/H1_with_uuid.parquet', '../data/H2_with_uuid.parquet']\n",
    "\n",
    "# save = True\n",
    "\n",
    "# for input_file, output_file in zip(input_files, output_files):\n",
    "#     df = db_utils.add_hotel_number_to_dataframe(input_file, output_file, save_to_parquet = save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Updated Parquets to DuckDB\n",
    "\n",
    "---\n",
    "\n",
    "After converting the source CSVs to parquet form, I will now create the database to be used in the rest of the project pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:07:45.729320Z",
     "iopub.status.busy": "2024-05-09T11:07:45.729320Z",
     "iopub.status.idle": "2024-05-09T11:07:46.340329Z",
     "shell.execute_reply": "2024-05-09T11:07:46.340329Z",
     "shell.execute_reply.started": "2024-05-09T11:07:45.729320Z"
    }
   },
   "outputs": [],
   "source": [
    "# # List of Parquet file paths\n",
    "# file_paths = ['../data/H1_with_uuid.parquet', '../data/H2_with_uuid.parquet']\n",
    "\n",
    "# # Path to the DuckDB database file\n",
    "# db_path = '../data/Hotel_reservations.duckdb'\n",
    "\n",
    "# # Check if the database file exists and remove it if it does\n",
    "# if os.path.exists(db_path):\n",
    "#     os.remove(db_path)\n",
    "\n",
    "# # Initialize connection to DuckDB\n",
    "# with duckdb.connect(database=db_path, read_only=False) as conn:\n",
    "    \n",
    "#     # Use the first file to create the table\n",
    "#     conn.execute(f\"CREATE TABLE source_data AS SELECT * FROM '{file_paths[0]}'\")\n",
    "    \n",
    "#     # For subsequent files, append data to the existing table\n",
    "#     for file_path in file_paths[1:]:  # Start from the second item\n",
    "#         conn.execute(f\"INSERT INTO source_data SELECT * FROM '{file_path}'\")\n",
    "       \n",
    "#     ## Confirm successful creation of database and table(s)\n",
    "#     display(conn.execute('SELECT * FROM source_data LIMIT 10').df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:17:12.473150Z",
     "iopub.status.busy": "2024-05-09T11:17:12.472149Z",
     "iopub.status.idle": "2024-05-09T11:17:12.500160Z",
     "shell.execute_reply": "2024-05-09T11:17:12.499150Z",
     "shell.execute_reply.started": "2024-05-09T11:17:12.473150Z"
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Interrupt workflow - check duckdb integrity/setup.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupt workflow - check duckdb integrity/setup.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Interrupt workflow - check duckdb integrity/setup."
     ]
    }
   ],
   "source": [
    "# raise Exception('Interrupt workflow - check duckdb integrity/setup.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:18:18.322938Z",
     "iopub.status.busy": "2024-05-09T11:18:18.321937Z",
     "iopub.status.idle": "2024-05-09T11:18:18.363931Z",
     "shell.execute_reply": "2024-05-09T11:18:18.362939Z",
     "shell.execute_reply.started": "2024-05-09T11:18:18.322938Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize connection to DuckDB\n",
    "# with duckdb.connect(database=db_path, read_only=True) as conn:\n",
    "       \n",
    "#     ## Confirm successful creation of database and table(s)w\n",
    "#     display(conn.execute('SELECT * FROM source_data WHERE HotelNumber = 2 LIMIT 10').df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T11:19:25.031257Z",
     "iopub.status.busy": "2024-05-09T11:19:25.030269Z",
     "iopub.status.idle": "2024-05-09T11:19:25.052260Z",
     "shell.execute_reply": "2024-05-09T11:19:25.051259Z",
     "shell.execute_reply.started": "2024-05-09T11:19:25.031257Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize connection to DuckDB\n",
    "# with duckdb.connect(database=db_path, read_only=True) as conn:\n",
    "       \n",
    "#     ## Confirm successful creation of database and table(s)w\n",
    "#     display(conn.execute('SELECT COUNT(HotelNumber) FROM source_data').df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Source Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to your DuckDB database\n",
    "# database_path = '../data/Hotel_reservations.duckdb'\n",
    "\n",
    "# # SQL command to copy the data from an existing table to a new table\n",
    "# copy_table_command = \"\"\"\n",
    "# CREATE TABLE res_data AS\n",
    "# SELECT * FROM source_data;\n",
    "# \"\"\"\n",
    "\n",
    "# with db_utils.duckdb_connection(database_path) as conn:\n",
    "#     conn.execute(copy_table_command)\n",
    "#     print(\"Table copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in file_paths:\n",
    "\n",
    "#     # Remove intermediate parquet files\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Updated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous workflow deleted these temporary files. However, a bug affecting the database creation process resulted in missing data. The concatenated data will serve as a replacement until the database is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_parquet(file_paths[0])\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_parquet(file_paths[1])\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_condensed = pd.concat([df1, df2], axis = 0)\n",
    "# df_condensed = df_condensed.reset_index(drop = True)\n",
    "# df_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Confirm correction of bug affecting one of the target features\n",
    "# df_condensed['IsCanceled'].value_counts(dropna= False, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_condensed.to_parquet('../data/data_condensed_with_uuid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds-env)",
   "language": "python",
   "name": "ds-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

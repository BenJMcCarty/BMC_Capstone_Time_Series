{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling - Regression\n",
    "\n",
    "---\n",
    "\n",
    "* Goal: to develop baseline models prior to feature engineering to compare performance vs. post-engineered models.\n",
    "\n",
    "My goal with this notebook is to develop a series of baseline models using minimal preprocessing. These models will establish a baseline performance for me to improve with additional feature engineering. Additionally, the most impactful features for each model can indicate if there are any features that are too strongly predictive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKLearn and Modeling Tools\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder, MeanEncoder\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.pipeline import Pipeline as fePipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, PowerTransformer, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "\n",
    "set_config(transform_output='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_feather('../../data/source/full_data.feather')\n",
    "# df_data = df_data.set_index('UUID')\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Target Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'ADR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.analyze(df_data,pairwise_analysis = 'off').show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Quick Overview: Review**\n",
    "\n",
    "Based on the quick EDA, I see there are both categorical features (several with high cardinality) and continuous (with right-tailed skews and some extreme outliers).\n",
    "\n",
    "**Questionable Features**\n",
    "\n",
    "First, I will drop the column `UUID` as it is a unique identifier and does not have any predictive value.\n",
    "\n",
    "There are two features that I can identify from domain knowledge as being too strongly predictive of the ADR (`IsCanceled`, `ReservationStatus`). These features indicate whether or not a guest stayed (if they cancel or no-show, the revenue is zero).\n",
    "\n",
    "Additionally, there are some temporal features that are either irrelevant to predictive modeling (`ArrivalDateYear`) or too closely related to the predictive features above (`ReservationStatusDate`).\n",
    "\n",
    "I will drop these features to match real-world data more closely/realistically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.drop(columns = ['UUID', 'IsCanceled',\n",
    "                                  'ReservationStatus',\n",
    "                                  'ReservationStatusDate',\n",
    "                                  'ArrivalDateYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "---\n",
    "\n",
    "***We know too much!***\n",
    "\n",
    "> The dataset contains several features which may not be knowable/available prior to the guest's stay.\n",
    "> For the purposes of revenue management forecasting, I need either to drop the features prior to modeling or, as an \"above and beyond\" goal, create models to forecast each of the unknowable features using the pure baseline features.\n",
    "\n",
    "**Features to Forget:**\n",
    "\n",
    "> * Meal\n",
    "> * IsRepeatedGuest\n",
    "> * PreviousCancellations\n",
    "> * PreviousBookingsNotCanceled\n",
    "> * AssignedRoomType\n",
    "> * BookingChanges\n",
    "> * DaysInWaitingList\n",
    "> * RequiredCarParkingSpaces\n",
    "> * TotalOfSpecialRequests\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_feats = ['Meal','IsRepeatedGuest','PreviousCancellations',\n",
    "              'PreviousBookingsNotCanceled','AssignedRoomType',\n",
    "              'BookingChanges','DaysInWaitingList',\n",
    "              'RequiredCarParkingSpaces','TotalOfSpecialRequests']\n",
    "\n",
    "df_data = df_data.drop(columns = drop_feats)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Inspection - Target\n",
    "\n",
    "---\n",
    "\n",
    "Based on my EDA, I noticed the target feature contains an extreme outlier of more than $5K. To improve my model performance, I will use the z-scores of each value to identify and remove outliers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[target_feature].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[target_feature].plot(kind = 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column to filter\n",
    "column_to_filter = 'ADR'  # Replace with the actual column name\n",
    "\n",
    "# Calculate z-scores for the specified column\n",
    "z_scores = np.abs(stats.zscore(df_data[column_to_filter]))\n",
    "\n",
    "# Define a threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Filter the data to remove outliers in the specified column\n",
    "df_data = df_data[z_scores < threshold]\n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[target_feature].plot(kind = 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data.drop(columns = target_feature)\n",
    "y = df_data[target_feature]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 903)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Modeling Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "Prior to performing any modeling, I will need to perform some prep work - namely filling missing values; encoding categorical features; handling outliers in numeric features; and scaling features (mostly for linear regression models).\n",
    "\n",
    "Before I fit my data using my preprocessing pipeline, I will transform my target feature using a Yeo-Johnson transformation. This will help reduce the impact of outliers in the target features without removing data entirely. Since scikit-learn's TransformedTargetRegressor class automatically inverts the transformation, the resulting predictions will be immediately interpretable.\n",
    "\n",
    "Finally, I will evaluate my model's performance based on the R^2 score; mean absolute error (\"MAE\"); median absolute error (\"MedAE\"); and the root mean squared error (\"RMSE\"). These metrics will give me several perspectives on the model's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical and numerical features\n",
    "cat_feats = X.select_dtypes(include=['object']).columns\n",
    "num_feats = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "## --- Create separate pipelines for categorical and numeric features --- ##\n",
    "\n",
    "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "                         ('encoder', CountFrequencyEncoder(unseen = 'encode',\n",
    "                                                           encoding_method = 'frequency',\n",
    "                                                           missing_values = 'ignore'))])\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='mean')),\n",
    "                         ('scaler', StandardScaler()),\n",
    "                         ('powertransformer', PowerTransformer(method = 'yeo-johnson'))])\n",
    "\n",
    "## --- Combine transformers into a single ColumnTransformer --- ##\n",
    "preprocessor = ColumnTransformer(transformers=[('num', num_pipeline, num_feats),\n",
    "                                               ('cat', cat_pipeline, cat_feats)])\n",
    "\n",
    "## ---  Create the TransformedTargetRegressor with Yeo-Johnson transformation --- ##\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "base_regressor = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "## --- Build the full pipeline --- ##\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('regressor', regressor)])\n",
    "\n",
    "## --- Fit the model and generate predictions --- ##\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "## --- Evaluate performance metrics --- ##\n",
    "score_training = model_pipeline.score(X_train, y_train)\n",
    "score_testing = model_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tweaks\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [tree.get_depth() for tree in model_pipeline[-1].regressor_.estimators_]\n",
    "\n",
    "sns.histplot(depths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical and numerical features\n",
    "cat_feats = X_train.select_dtypes(include=['object']).columns\n",
    "num_feats = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Create separate pipelines for categorical and numeric features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', CountFrequencyEncoder(unseen='encode',\n",
    "                                      encoding_method='frequency',\n",
    "                                      missing_values='ignore'))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('winsorizer', Winsorizer(capping_method='gaussian', tail='both', fold=3)),\n",
    "    ('powertransformer', PowerTransformer(method='yeo-johnson'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, num_feats),\n",
    "    ('cat', cat_pipeline, cat_feats)\n",
    "])\n",
    "\n",
    "# Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "base_regressor = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "# Build the full pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Set up hyperparameter tuning with HalvingGridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__regressor__max_depth': [15,20],\n",
    "    'regressor__regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__regressor__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "halving_grid_search = HalvingGridSearchCV(model_pipeline,\n",
    "                                          param_grid,\n",
    "                                        #   scoring='neg_median_absolute_error',\n",
    "                                          cv=3,\n",
    "                                          n_jobs=-1,\n",
    "                                          factor=2,\n",
    "                                          min_resources=\"exhaust\")\n",
    "\n",
    "# Fit the model and generate predictions\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = halving_grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance metrics\n",
    "score_training = best_model.score(X_train, y_train)\n",
    "score_testing = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "median_ae = metrics.median_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Select categorical and numerical features\n",
    "# cat_feats = X.select_dtypes(include=['object']).columns\n",
    "# num_feats = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# ## --- Create separate pipelines for categorical and numeric features --- ##\n",
    "\n",
    "# cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "#                          ('encoder', CountFrequencyEncoder(unseen = 'encode',\n",
    "#                                                            encoding_method = 'frequency',\n",
    "#                                                            missing_values = 'ignore'))])\n",
    "\n",
    "# num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='mean')),\n",
    "#                          ('powertransformer', PowerTransformer(method = 'yeo-johnson')),\n",
    "#                          ('scaler', StandardScaler())])\n",
    "\n",
    "# ## --- Combine transformers into a single ColumnTransformer --- ##\n",
    "# preprocessor = ColumnTransformer(transformers=[('num', num_pipeline, num_feats),\n",
    "#                                                ('cat', cat_pipeline, cat_feats)])\n",
    "\n",
    "# ## ---  Create the TransformedTargetRegressor with Yeo-Johnson transformation --- ##\n",
    "# target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# base_regressor = RandomForestRegressor()\n",
    "\n",
    "# regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "#                                        transformer=target_transformer)\n",
    "\n",
    "# ## --- Build the full pipeline --- ##\n",
    "# model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                                  ('regressor', regressor)])\n",
    "\n",
    "# ## --- Set up hyperparameter tuning with GridSearchCV --- ##\n",
    "# param_grid = {\n",
    "#       'regressor__regressor__max_depth': [25, 45],\n",
    "#       'regressor__regressor__min_samples_split': [2, 3],\n",
    "#       'regressor__regressor__min_samples_leaf': [2, 4]}\n",
    "\n",
    "# grid_search = GridSearchCV(model_pipeline,\n",
    "#                            param_grid,\n",
    "#                            scoring='neg_median_absolute_error',\n",
    "#                            cv=3,\n",
    "#                            n_jobs=-1)\n",
    "\n",
    "# ## --- Fit the model and generate predictions --- ##\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# ## --- Evaluate performance metrics --- ##\n",
    "# score_training = best_model.score(X_train, y_train)\n",
    "# score_testing = best_model.score(X_test, y_test)\n",
    "\n",
    "# print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "#       f'Testing Score: {score_testing:,.3f}\\n'\n",
    "#       f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "# mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "# median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "# mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "# rmse = np.sqrt(mse)\n",
    "\n",
    "\n",
    "# print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "#       f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "#       f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The average actual ADR is ${y_test.mean().values[0].round(2)}.\\n'\n",
    "      f'''The model's predictions are off by about {round(median_ae/y_test.mean().values[0]*100,2)}%''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inspection - Permutation Importances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importances\n",
    "result = permutation_importance(grid_search.best_estimator_,\n",
    "                                X_test, y_test,\n",
    "                                scoring = 'neg_median_absolute_error',\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Extract importances and standard deviations\n",
    "perm_importances = result.importances_mean\n",
    "perm_importances_std = result.importances_std\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': perm_importances,\n",
    "    'Importance_std': perm_importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "sns.barplot(x='Importance', y='Feature',data=importance_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(grid_search.best_estimator_,\n",
    "            '../../data/baseline_model.joblib',\n",
    "            compress = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = joblib.load('../../data/baseline_model.joblib')\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical and numerical features\n",
    "cat_feats = X_train.select_dtypes(include=['object']).columns\n",
    "num_feats = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Create separate pipelines for categorical and numeric features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', CountFrequencyEncoder(unseen='encode', encoding_method='frequency', missing_values='ignore'))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('powertransformer', PowerTransformer(method='yeo-johnson')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine transformers into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, num_feats),\n",
    "    ('cat', cat_pipeline, cat_feats)\n",
    "])\n",
    "\n",
    "# Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "base_regressor = DecisionTreeRegressor()\n",
    "\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor, transformer=target_transformer)\n",
    "\n",
    "# Build the full pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Set up hyperparameter tuning with HalvingGridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__regressor__max_depth': [5, 10, 20, 30],\n",
    "    'regressor__regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__regressor__min_samples_leaf': [2, 4, 8],\n",
    "    'regressor__regressor__max_features': ['sqrt', 'log2', 0.5]\n",
    "}\n",
    "\n",
    "halving_grid_search = HalvingGridSearchCV(model_pipeline,\n",
    "                                          param_grid,\n",
    "                                          scoring='neg_median_absolute_error',\n",
    "                                          cv=3,\n",
    "                                          n_jobs=-1,\n",
    "                                          factor=2,\n",
    "                                          random_state=42)\n",
    "\n",
    "# Fit the model and generate predictions\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = halving_grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance metrics\n",
    "score_training = best_model.score(X_train, y_train)\n",
    "score_testing = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "median_ae = metrics.median_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DummyRegressor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target transformer\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Instantiate the model\n",
    "base_regressor = DummyRegressor()\n",
    "\n",
    "# Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "# Build the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "score_training = model_pipeline.score(X_train, y_train)\n",
    "score_testing = model_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.log2(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Create separate pipelines for categorical and numeric features --- ##\n",
    "\n",
    "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "                         ('encoder', CountFrequencyEncoder(unseen = 'encode',\n",
    "                                                           encoding_method = 'frequency',\n",
    "                                                           missing_values = 'ignore'))])\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='mean')),\n",
    "                         ('scaler', StandardScaler()),\n",
    "                         ('powertransformer', PowerTransformer(method = 'yeo-johnson'))])\n",
    "\n",
    "## --- Combine transformers into a single ColumnTransformer --- ##\n",
    "preprocessor = ColumnTransformer(transformers=[('num', num_pipeline, num_feats),\n",
    "                                               ('cat', cat_pipeline, cat_feats)])\n",
    "\n",
    "## ---  Create the TransformedTargetRegressor with Yeo-Johnson transformation --- ##\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "base_regressor = HistGradientBoostingRegressor(loss = 'absolute_error')\n",
    "\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "## --- Build the full pipeline --- ##\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('regressor', regressor)])\n",
    "\n",
    "## --- Fit the model and generate predictions --- ##\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "## --- Evaluate performance metrics --- ##\n",
    "score_training = model_pipeline.score(X_train, y_train)\n",
    "score_testing = model_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target transformer\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Instantiate the model\n",
    "base_regressor = HistGradientBoostingRegressor(random_state = 903)\n",
    "\n",
    "# Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "# Build the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "score_training = model_pipeline.score(X_train, y_train)\n",
    "score_testing = model_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target transformer\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Instantiate the model\n",
    "base_regressor = SGDRegressor(loss='huber',penalty='elasticnet')\n",
    "\n",
    "# Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "# Build the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "score_training = model_pipeline.score(X_train, y_train)\n",
    "score_testing = model_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.log2(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target transformer\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Instantiate the model\n",
    "base_regressor = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "                                       transformer=target_transformer)\n",
    "\n",
    "# Build the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "score_training = model_pipeline.score(X_train, y_train)\n",
    "score_testing = model_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.log2(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target transformer\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# # Instantiate the model\n",
    "# base_regressor = XGBRFRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Instantiate the model\n",
    "regressor = XGBRFRegressor(objective='reg:squarederror')\n",
    "\n",
    "# # Create the TransformedTargetRegressor with Yeo-Johnson transformation\n",
    "# regressor = TransformedTargetRegressor(regressor=base_regressor,\n",
    "#                                        transformer=target_transformer)\n",
    "\n",
    "# Build the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__subsample': [0.5, 0.7, 1.0],\n",
    "    'regressor__colsample_bytree': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(model_pipeline,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='neg_median_absolute_error',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = grid_search.predict(X_test)\n",
    "score_training = grid_search.score(X_train, y_train)\n",
    "score_testing = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f'\\nTraining Score: {score_training:,.3f}\\n'\n",
    "      f'Testing Score: {score_testing:,.3f}\\n'\n",
    "      f'Difference: {score_training - score_testing:,.3f}\\n')\n",
    "\n",
    "mean_ae = metrics.mean_absolute_error(y_pred, y_test)\n",
    "median_ae = metrics.median_absolute_error(y_pred, y_test)\n",
    "mse = metrics.mean_squared_error(y_pred, y_test)\n",
    "rmse = np.log2(mse)\n",
    "\n",
    "\n",
    "print(f'The MAE is ${mean_ae:,.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:,.2f}\\n'\n",
    "      f'The RMSE is ${rmse:,.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data.drop(columns = target_feature)\n",
    "y = df_data[target_feature]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features\n",
    "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "                         ('encoder', CountFrequencyEncoder(unseen = 'encode',\n",
    "                                                           encoding_method = 'frequency',\n",
    "                                                           missing_values = 'ignore'))])\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='mean')),\n",
    "                         ('powertransformer', PowerTransformer(method='yeo-johnson')),\n",
    "                         ('scaler', MinMaxScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the features\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Perform Yeo-Johnson transformation on the target\n",
    "target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "y_train_transformed = target_transformer.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_transformed = target_transformer.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Convert data to TensorFlow tensors\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_train_transformed = tf.convert_to_tensor(y_train_transformed, dtype=tf.float32)\n",
    "y_test_transformed = tf.convert_to_tensor(y_test_transformed, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the single-layer neural network model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.InputLayer(shape=(X_train.shape[1],)),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "        # Build the single-layer neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        # tf.keras.layers.Dense(16, activation='relu'),\n",
    "        # tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile the model with the Adam optimizer and a learning rate scheduler\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='mean_absolute_error')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install dojo-ds\n",
    "# import dojo_ds as ds \n",
    "# help(ds.evaluate.plot_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = build_model()\n",
    "history = model.fit(X_train, y_train, epochs=75, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_train, y_train, verbose=1)\n",
    "print(f'Mean Absolute Error on Training Data: {loss:.2f}')\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Mean Absolute Error on Test Data: {loss:.2f}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance metrics\n",
    "mean_ae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "median_ae = metrics.median_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'\\nThe MAE is ${mean_ae:.2f}\\n'\n",
    "      f'The MedAE is ${median_ae:.2f}\\n'\n",
    "      f'The RMSE is ${rmse:.2f}\\n')\n",
    "\n",
    "# ds.evaluate.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.plot(marker = 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results\n",
    "\n",
    "---\n",
    "\n",
    "The best model was the Random Forest Regressor model. This model performed well with minor preprocessing, leading me to believe there may be features that are strongly predictive of the ADR. I will need to investigate further to confirm.\n",
    "\n",
    "My next steps will be to perform additional EDA and feature engineering. During the EDA process, I will identify and remove any features that would not be known before the guest's stay. Then, I will engineer new features, including temporal features related to booking and stay dates, as well as hotel occupancy rates.\n",
    "\n",
    "The end goal is to ensure my final dataset properly represents reservations prior to the guest stay and includes additional detail not present in the initial dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds-env)",
   "language": "python",
   "name": "ds-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

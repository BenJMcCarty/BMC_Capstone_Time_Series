{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quack Quack - Creating the DuckDB\n",
    "\n",
    "---\n",
    "\n",
    "**Ducking the Data with a Database**\n",
    "\n",
    "As this project will involve many data transformations; engineered features; and iterative modeling; I need an orderly, robust system to handle all of the data, models, etc. without creating too much complexity in the repository. Instead of creating separate files for each version of the data, I decided that I need to create a small database to store the information  effectively and efficiently. Before I can create the database, I need data!\n",
    "\n",
    "**Hatching the Plan**\n",
    "\n",
    "I obtained my source data from the article referenced in the `README.md` file. The source data comes in the form of two separate CSV files, which are both sizeable and take a while to load into a dataframe (or in this case, a database). To reduce the size and increase read/write times, I will convert the original source files from CSVs to parquet files. Then, I will take the raw reservation data and use it to create the first table within the database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enabling access to custom functions in separate directory\n",
    "\n",
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Construct the absolute path to the 'src' directory\n",
    "src_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "\n",
    "# Append the path to 'sys.path'\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Source CSVs to Parquet\n",
    "\n",
    "---\n",
    "\n",
    "The following code loops through this repository's `/data/` directory; searches for the source CSVs; converts each of them to a parquet file; and then deletes the CSV.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = '../data/source/'\n",
    "\n",
    "# Pattern match for files named 'h1.csv' or 'h2.csv'\n",
    "file_patterns = [os.path.join(directory, 'h1.csv'), os.path.join(directory, 'h2.csv')]\n",
    "\n",
    "# Initialize a list to hold the matched file paths\n",
    "csv_files = []\n",
    "\n",
    "# Loop through the patterns and extend the list with found files\n",
    "for pattern in file_patterns:\n",
    "    csv_files.extend(glob.glob(pattern))\n",
    "\n",
    "# Check if no files were found\n",
    "if not csv_files:\n",
    "    print(\"No matching filepaths found. Stopping execution.\")\n",
    "else:\n",
    "    # Loop through each found CSV file\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            df['HotelNumber'] = csv_file[1:2]\n",
    "            \n",
    "            # Define the Parquet file path (same name as the CSV file but with .parquet extension)\n",
    "            parquet_file = csv_file.replace('.csv', '.parquet')\n",
    "            \n",
    "            # Convert the DataFrame to a Parquet file\n",
    "            df.to_parquet(parquet_file)\n",
    "\n",
    "            # If the conversion was successful, remove the CSV file\n",
    "            os.remove(csv_file)\n",
    "            print(f\"Successfully converted and removed {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {csv_file}: {e}\")\n",
    "\n",
    "    print(\"Conversion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Append UUIDs\n",
    "\n",
    "Since the source data was anonymized, there are no unique identifiers for each reservation. To support database joins and relationships between tables, I will add columns for both a UUID and the source hotel number to differentiate the reservations and preserve the unique details of each hotel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['../data/source/H1.parquet', '../data/source/H2.parquet']\n",
    "output_files = ['../data/H1_with_uuid.parquet', '../data/H2_with_uuid.parquet']\n",
    "\n",
    "save = True\n",
    "\n",
    "for input, output in zip(input_files, output_files):\n",
    "    df = db_utils.add_hotel_number_to_dataframe(input,output, save_to_parquet = save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Updated Parquets to DuckDB\n",
    "\n",
    "---\n",
    "\n",
    "After converting the source CSVs to parquet form, I will now create the database to be used in the rest of the project pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Parquet file paths\n",
    "file_paths = ['../data/H1_with_uuid.parquet', '../data/H2_with_uuid.parquet']\n",
    "\n",
    "# Path to the DuckDB database file\n",
    "db_path = '../data/Hotel_reservations.duckdb'\n",
    "\n",
    "# Check if the database file exists and remove it if it does\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "# Initialize connection to DuckDB\n",
    "conn = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "# Use the first file to create the table\n",
    "conn.execute(f\"CREATE TABLE source_data AS SELECT * FROM '{file_paths[0]}'\")\n",
    "\n",
    "# For subsequent files, append data to the existing table\n",
    "for file_path in file_paths[1:]:  # Start from the second item\n",
    "    conn.execute(f\"INSERT INTO source_data SELECT * FROM '{file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "## Confirm successful creation of database and table(s)\n",
    "display(conn.execute('SELECT * FROM source_data LIMIT 10').df())\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Source Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your DuckDB database\n",
    "database_path = '../data/Hotel_reservations.duckdb'\n",
    "\n",
    "# SQL command to copy the data from an existing table to a new table\n",
    "copy_table_command = \"\"\"\n",
    "CREATE TABLE res_data AS\n",
    "SELECT * FROM source_data;\n",
    "\"\"\"\n",
    "\n",
    "with db_utils.duckdb_connection(database_path) as conn:\n",
    "    conn.execute(copy_table_command)\n",
    "    print(\"Table copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in file_paths:\n",
    "\n",
    "#     # Remove intermediate parquet files\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Updated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous workflow deleted these temporary files. However, a bug affecting the database creation process resulted in missing data. The concatenated data will serve as a replacement until the database is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet(file_paths[0])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet(file_paths[1])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condensed = pd.concat([df1, df2], axis = 0)\n",
    "df_condensed = df_condensed.reset_index(drop = True)\n",
    "df_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm correction of bug affecting one of the target features\n",
    "df_condensed['IsCanceled'].value_counts(dropna= False, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condensed.to_parquet('../data/data_condensed_with_uuid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
